{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CART-POLE GYM ENVIRONMENT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our aim is to devise RL algorithms to try and solve the \"Cart-Pole\" environment provided in the Open-AI gym. In this environment, a pole is attached to a cart which can move left or right. Our goal is to apply forces to the cart so as to keep it upright. An episode runs a maximum for 500 time-steps, hence if the pole stays upright for 500 time-steps, the episode is considered solved. The reward for each time-step is 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The action space for this environment is a ndarray with shape (1,) which can take values {0,1} indicating the direction of the fixed force the cart is pushed with. The observation space is a ndarray with shape (4,) which contains the following values:\n",
    "| Num | Observation | Min | Max |\n",
    "|-----------------|-----------------| -----|-----|\n",
    "| 0  | Cart Position  | -4.8 | 4.8 |\n",
    "| 1  | Cart Velocity  | -Inf | Inf |\n",
    "| 2  | Pole Angle     | -24 deg | 24 deg |\n",
    "| 3  | Pole Angular Velocity  | -Inf | Inf |\n",
    "\n",
    "Also, the episode terminates if the cart position leaves the range [-2.4, 2.4] or the pole angle leaves the range [-12 deg, 12 deg] and the pole angle is returned in radians."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the State Space is continous, hence we cannot just use a lookup table to store our Q-values. There are two ways we can counter this, one is we use a function approximator with some parameters to approximate our Q-values. Another way is to discretise the state space, as to break it into a discrete space rather than being continous. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To discretize the state space we will split each observation into some n equal parts and assign each interval to its lower bound. Now for obs number 0 and 2 we can discretize in a way that if the values are outside their non terminal range, we just put all of those into one interval rather than splitting them but as of now we aren't doing that, maybe we'll try later to see if it optimises our space much. (As in our original implementation a lot of the states won't ever be reached as the episode terminates before reaching them). For observation 1 and 3, they go from -INF to +INF, hence we can't just split it into some n intervals and get a finite state space. For that what we did was we ran the environment many times by hand to get an idea of the range of values the velocity and angular velocity stay in before the episode terminates because the velocity or angular velocity we just to large enough to be able to correct. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this code, we did some experimentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "env = gym.make(\"CartPole-v1\" , render_mode=\"rgb_array\")\n",
    "env.reset()\n",
    "while True:\n",
    "    action = int(input(\"Action: \"))\n",
    "    if action in (0, 1):\n",
    "        x = env.step(action)\n",
    "        print(f\"v:{x[0][1]} , w:{x[0][3]}\")\n",
    "        env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this we got to the conclusion that velocity takes values between -3.0 and 3.0 and angular velocity stays between -4 and 4 at max. Also if the values go outside this interval, we equate them to these bounds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doing some pretty basic mathematics we see that we can implementing this by mapping our observation x to $$round(\\frac{x-a}{b-a} \\times n)$$ where n is the \"granularity constant\" and a and b are the lower and upper bounds of x respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now coming to the RL algorithm to be used, we see all episodes are guaranteed to be terminated, hence we can use any either forward view or backward view algorithm. Here we'll be implementing backward view SARSA($\\lambda$) with Eligibility Traces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we import the required modules we'll be using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll define some constants which we'll be using throughout such as learning rate, discount factor etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "granularity_0 = 25\n",
    "granularity_1 = 25\n",
    "granularity_2 = 25\n",
    "granularity_3 = 25\n",
    "\n",
    "EPSILON_INIT = 1\n",
    "EPSILON_DECAY = 0.9999\n",
    "EPSILON_MIN = 0.05\n",
    "EPSILON = EPSILON_INIT\n",
    "\n",
    "DISCOUNT_FACTOR = 0.99\n",
    "\n",
    "LEARNING_RATE = 0.9\n",
    "LEARNING_RATE_DECAY = 0.99999\n",
    "\n",
    "LAMBDA = 0.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also initialise some other variables and our q tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = np.zeros(( granularity_0 , granularity_1 , granularity_2 , granularity_3  , 2))\n",
    "n = 3000\n",
    "reward_list = np.zeros(n+1)\n",
    "ep_count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_action(state):\n",
    "    global EPSILON\n",
    "    EPSILON = max(EPSILON_MIN , EPSILON * EPSILON_DECAY)\n",
    "    if(np.random.random() < EPSILON):\n",
    "        return int(env.action_space.sample())\n",
    "    else:\n",
    "        return np.argmax(Q[int(state[0]) , int(state[1]), int(state[2]) , int(state[3]) , :])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discretize_state(state):\n",
    "    state[0] = int(((state[0] + 4.8) / (9.6)) * granularity_0 )\n",
    "    state[1] = int(((state[1] + 3.0) / (6.0)) * granularity_1 )\n",
    "    state[2] = int(((state[2] + 0.42) / (0.84)) * granularity_2 )\n",
    "    state[3] = int(((state[3] + 4.0) / (8.0)) * granularity_3 )\n",
    "    return (state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The choose_action function implements our epsilon-greedy algorithm and the discretize_states function discretizes our state space based on the corresponding granularity constant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\" , render_mode=\"rgb_array\")\n",
    "\n",
    "while(n):\n",
    "    n -= 1\n",
    "    ep_count += 1\n",
    "    E = np.zeros(( granularity_0 , granularity_1 , granularity_2 , granularity_3  , 2))\n",
    "    state_prev = env.reset()[0]\n",
    "    state_prev = discretize_state(state_prev)\n",
    "    action_prev = env.action_space.sample()\n",
    "    total_reward = 0\n",
    "    while(True):\n",
    "        state, reward, done, truncated, info = env.step(action_prev)\n",
    "        total_reward += reward\n",
    "        state = discretize_state(state)\n",
    "        action = choose_action(state)\n",
    "        Td_error = reward + (DISCOUNT_FACTOR * Q[int(state[0]) , int(state[1]), int(state[2]) , int(state[3]) , action]) - Q[int(state_prev[0]) , int(state_prev[1]), int(state_prev[2]) , int(state_prev[3]) , action_prev]\n",
    "        E[int(state_prev[0]) , int(state_prev[1]), int(state_prev[2]) , int(state_prev[3]) , action_prev] += 1\n",
    "        Q += LEARNING_RATE * Td_error * E\n",
    "        E *= (DISCOUNT_FACTOR * LAMBDA)\n",
    "        state_prev = state\n",
    "        action_prev = action\n",
    "        if done:\n",
    "            break\n",
    "    LEARNING_RATE = max(0.1 , LEARNING_RATE * LEARNING_RATE_DECAY)\n",
    "    reward_list[ep_count] = total_reward\n",
    "    print(f\"total_reward : {total_reward} , epsilon : {EPSILON} , episode : {ep_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
